{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/Volviane/Cats_Dogs_classification/blob/master/transfer_learning_dog_cat_classification_keras.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CPR6GHBRpMG1"
   },
   "source": [
    "# Cat and dog classification with Transfer learning with Tensorflow Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2NlVPATTpBX2"
   },
   "source": [
    "We have  built a nifty utility module which we will be using to evaluate the performance of our deep learning models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yfrp0VYsCmLx"
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Mon Jul 31 20:05:23 2017\n",
    "\n",
    "@author: DIP\n",
    "@Copyright: Dipanjan Sarkar\n",
    "\"\"\"\n",
    "\n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.base import clone\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from scipy import interp\n",
    "from sklearn.metrics import roc_curve, auc \n",
    "\n",
    "\n",
    "def get_metrics(true_labels, predicted_labels):\n",
    "    \n",
    "    print('Accuracy:', np.round(\n",
    "                        metrics.accuracy_score(true_labels, \n",
    "                                               predicted_labels),\n",
    "                        4))\n",
    "    print('Precision:', np.round(\n",
    "                        metrics.precision_score(true_labels, \n",
    "                                               predicted_labels,\n",
    "                                               average='weighted'),\n",
    "                        4))\n",
    "    print('Recall:', np.round(\n",
    "                        metrics.recall_score(true_labels, \n",
    "                                               predicted_labels,\n",
    "                                               average='weighted'),\n",
    "                        4))\n",
    "    print('F1 Score:', np.round(\n",
    "                        metrics.f1_score(true_labels, \n",
    "                                               predicted_labels,\n",
    "                                               average='weighted'),\n",
    "                        4))\n",
    "                        \n",
    "\n",
    "def train_predict_model(classifier, \n",
    "                        train_features, train_labels, \n",
    "                        test_features, test_labels):\n",
    "    # build model    \n",
    "    classifier.fit(train_features, train_labels)\n",
    "    # predict using model\n",
    "    predictions = classifier.predict(test_features) \n",
    "    return predictions    \n",
    "\n",
    "\n",
    "def display_confusion_matrix(true_labels, predicted_labels, classes=[1,0]):\n",
    "    \n",
    "    total_classes = len(classes)\n",
    "    level_labels = [total_classes*[0], list(range(total_classes))]\n",
    "\n",
    "    cm = metrics.confusion_matrix(y_true=true_labels, y_pred=predicted_labels, \n",
    "                                  labels=classes)\n",
    "    cm_frame = pd.DataFrame(data=cm, \n",
    "                            columns=pd.MultiIndex(levels=[['Predicted:'], classes], \n",
    "                                                  labels=level_labels), \n",
    "                            index=pd.MultiIndex(levels=[['Actual:'], classes], \n",
    "                                                labels=level_labels)) \n",
    "    print(cm_frame) \n",
    "    \n",
    "def display_classification_report(true_labels, predicted_labels, classes=[1,0]):\n",
    "\n",
    "    report = metrics.classification_report(y_true=true_labels, \n",
    "                                           y_pred=predicted_labels, \n",
    "                                           labels=classes) \n",
    "    print(report)\n",
    "    \n",
    "    \n",
    "    \n",
    "def display_model_performance_metrics(true_labels, predicted_labels, classes=[1,0]):\n",
    "    print('Model Performance metrics:')\n",
    "    print('-'*30)\n",
    "    get_metrics(true_labels=true_labels, predicted_labels=predicted_labels)\n",
    "    print('\\nModel Classification report:')\n",
    "    print('-'*30)\n",
    "    display_classification_report(true_labels=true_labels, predicted_labels=predicted_labels, \n",
    "                                  classes=classes)\n",
    "    print('\\nPrediction Confusion Matrix:')\n",
    "    print('-'*30)\n",
    "    display_confusion_matrix(true_labels=true_labels, predicted_labels=predicted_labels, \n",
    "                             classes=classes)\n",
    "\n",
    "\n",
    "def plot_model_decision_surface(clf, train_features, train_labels,\n",
    "                                plot_step=0.02, cmap=plt.cm.RdYlBu,\n",
    "                                markers=None, alphas=None, colors=None):\n",
    "    \n",
    "    if train_features.shape[1] != 2:\n",
    "        raise ValueError(\"X_train should have exactly 2 columnns!\")\n",
    "    \n",
    "    x_min, x_max = train_features[:, 0].min() - plot_step, train_features[:, 0].max() + plot_step\n",
    "    y_min, y_max = train_features[:, 1].min() - plot_step, train_features[:, 1].max() + plot_step\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, plot_step),\n",
    "                         np.arange(y_min, y_max, plot_step))\n",
    "\n",
    "    clf_est = clone(clf)\n",
    "    clf_est.fit(train_features,train_labels)\n",
    "    if hasattr(clf_est, 'predict_proba'):\n",
    "        Z = clf_est.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:,1]\n",
    "    else:\n",
    "        Z = clf_est.predict(np.c_[xx.ravel(), yy.ravel()])    \n",
    "    Z = Z.reshape(xx.shape)\n",
    "    cs = plt.contourf(xx, yy, Z, cmap=cmap)\n",
    "    \n",
    "    le = LabelEncoder()\n",
    "    y_enc = le.fit_transform(train_labels)\n",
    "    n_classes = len(le.classes_)\n",
    "    plot_colors = ''.join(colors) if colors else [None] * n_classes\n",
    "    label_names = le.classes_\n",
    "    markers = markers if markers else [None] * n_classes\n",
    "    alphas = alphas if alphas else [None] * n_classes\n",
    "    for i, color in zip(range(n_classes), plot_colors):\n",
    "        idx = np.where(y_enc == i)\n",
    "        plt.scatter(train_features[idx, 0], train_features[idx, 1], c=color,\n",
    "                    label=label_names[i], cmap=cmap, edgecolors='black', \n",
    "                    marker=markers[i], alpha=alphas[i])\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_model_roc_curve(clf, features, true_labels, label_encoder=None, class_names=None):\n",
    "    \n",
    "    ## Compute ROC curve and ROC area for each class\n",
    "    fpr = dict()\n",
    "    tpr = dict()\n",
    "    roc_auc = dict()\n",
    "    if hasattr(clf, 'classes_'):\n",
    "        class_labels = clf.classes_\n",
    "    elif label_encoder:\n",
    "        class_labels = label_encoder.classes_\n",
    "    elif class_names:\n",
    "        class_labels = class_names\n",
    "    else:\n",
    "        raise ValueError('Unable to derive prediction classes, please specify class_names!')\n",
    "    n_classes = len(class_labels)\n",
    "    y_test = label_binarize(true_labels, classes=class_labels)\n",
    "    if n_classes == 2:\n",
    "        if hasattr(clf, 'predict_proba'):\n",
    "            prob = clf.predict_proba(features)\n",
    "            y_score = prob[:, prob.shape[1]-1] \n",
    "        elif hasattr(clf, 'decision_function'):\n",
    "            prob = clf.decision_function(features)\n",
    "            y_score = prob[:, prob.shape[1]-1]\n",
    "        else:\n",
    "            raise AttributeError(\"Estimator doesn't have a probability or confidence scoring system!\")\n",
    "        \n",
    "        fpr, tpr, _ = roc_curve(y_test, y_score)      \n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        plt.plot(fpr, tpr, label='ROC curve (area = {0:0.2f})'\n",
    "                                 ''.format(roc_auc),\n",
    "                 linewidth=2.5)\n",
    "        \n",
    "    elif n_classes > 2:\n",
    "        if hasattr(clf, 'predict_proba'):\n",
    "            y_score = clf.predict_proba(features)\n",
    "        elif hasattr(clf, 'decision_function'):\n",
    "            y_score = clf.decision_function(features)\n",
    "        else:\n",
    "            raise AttributeError(\"Estimator doesn't have a probability or confidence scoring system!\")\n",
    "\n",
    "        for i in range(n_classes):\n",
    "            fpr[i], tpr[i], _ = roc_curve(y_test[:, i], y_score[:, i])\n",
    "            roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "        ## Compute micro-average ROC curve and ROC area\n",
    "        fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_test.ravel(), y_score.ravel())\n",
    "        roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "\n",
    "        ## Compute macro-average ROC curve and ROC area\n",
    "        # First aggregate all false positive rates\n",
    "        all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))\n",
    "        # Then interpolate all ROC curves at this points\n",
    "        mean_tpr = np.zeros_like(all_fpr)\n",
    "        for i in range(n_classes):\n",
    "            mean_tpr += interp(all_fpr, fpr[i], tpr[i])\n",
    "        # Finally average it and compute AUC\n",
    "        mean_tpr /= n_classes\n",
    "        fpr[\"macro\"] = all_fpr\n",
    "        tpr[\"macro\"] = mean_tpr\n",
    "        roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
    "\n",
    "        ## Plot ROC curves\n",
    "        plt.figure(figsize=(6, 4))\n",
    "        plt.plot(fpr[\"micro\"], tpr[\"micro\"],\n",
    "                 label='micro-average ROC curve (area = {0:0.2f})'\n",
    "                       ''.format(roc_auc[\"micro\"]), linewidth=3)\n",
    "\n",
    "        plt.plot(fpr[\"macro\"], tpr[\"macro\"],\n",
    "                 label='macro-average ROC curve (area = {0:0.2f})'\n",
    "                       ''.format(roc_auc[\"macro\"]), linewidth=3)\n",
    "\n",
    "        for i, label in enumerate(class_labels):\n",
    "            plt.plot(fpr[i], tpr[i], label='ROC curve of class {0} (area = {1:0.2f})'\n",
    "                                           ''.format(label, roc_auc[i]), \n",
    "                     linewidth=2, linestyle=':')\n",
    "    else:\n",
    "        raise ValueError('Number of classes should be atleast 2 or more')\n",
    "        \n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oJylivL6pgj6"
   },
   "source": [
    "# Load the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sx8yRx6Ve88l"
   },
   "outputs": [],
   "source": [
    "# Load helper file\n",
    "link = \"https://drive.google.com/file/d/1Cn0B9Zr2irUnZcHqODT9IilGHf9fZ61R/view?usp=sharing\"\n",
    "\n",
    "_, id_t = link.split('d/')\n",
    "\n",
    "id = id_t.split('/')[0]\n",
    "\n",
    "print (\"Loading file ...\")\n",
    "\n",
    "print (id) # Verify that you have everything after '='\n",
    "\n",
    "# Install the PyDrive wrapper & import libraries.\n",
    "# This only needs to be done once per notebook.\n",
    "!pip install -U -q PyDrive\n",
    "from pydrive.auth import GoogleAuth\n",
    "from pydrive.drive import GoogleDrive\n",
    "from google.colab import auth\n",
    "from oauth2client.client import GoogleCredentials\n",
    "\n",
    "# Authenticate and create the PyDrive client.\n",
    "# This only needs to be done once per notebook.\n",
    "auth.authenticate_user()\n",
    "gauth = GoogleAuth()\n",
    "gauth.credentials = GoogleCredentials.get_application_default()\n",
    "drive = GoogleDrive(gauth)\n",
    "\n",
    "\n",
    "file_id = id\n",
    "downloaded = drive.CreateFile({'id':file_id})\n",
    "downloaded.FetchMetadata(fetch_all=True)\n",
    "downloaded.GetContentFile(downloaded.metadata['title'])\n",
    "print (\"Completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qmKCWRF5fSiy"
   },
   "outputs": [],
   "source": [
    "!unzip Cat_Dog_data.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "45Wan8N_f1zt"
   },
   "outputs": [],
   "source": [
    "!ls Cat_Dog_data/train/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ASTh5lnXppH4"
   },
   "source": [
    "## Import the package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-Lp6W6UddfLq"
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import numpy as np\n",
    "import os\n",
    "import shutil\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array, array_to_img\n",
    "%matplotlib inline\n",
    "\n",
    "# encode text category labels\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from keras.models import Sequential\n",
    "from keras import optimizers\n",
    "\n",
    "from keras.applications import vgg16\n",
    "from keras.models import Model\n",
    "import keras\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4h7XfxGQpwwe"
   },
   "source": [
    "## Building Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3G8Z36Yyf1Qb"
   },
   "outputs": [],
   "source": [
    "files = glob.glob('Cat_Dog_data/train/cat/*')\n",
    "files1 = glob.glob('Cat_Dog_data/train/dog/*')\n",
    "\n",
    "cat_files = [fn for fn in files if 'cat' in fn]\n",
    "dog_files = [fn for fn in files1 if 'dog' in fn]\n",
    "len(cat_files), len(dog_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8cSbWealeEBd"
   },
   "outputs": [],
   "source": [
    "cat_train = np.random.choice(cat_files, size=1500, replace=False)\n",
    "dog_train = np.random.choice(dog_files, size=1500, replace=False)\n",
    "cat_files = list(set(cat_files) - set(cat_train))\n",
    "dog_files = list(set(dog_files) - set(dog_train))\n",
    "\n",
    "cat_val = np.random.choice(cat_files, size=500, replace=False)\n",
    "dog_val = np.random.choice(dog_files, size=500, replace=False)\n",
    "cat_files = list(set(cat_files) - set(cat_val))\n",
    "dog_files = list(set(dog_files) - set(dog_val))\n",
    "\n",
    "cat_test = np.random.choice(cat_files, size=500, replace=False)\n",
    "dog_test = np.random.choice(dog_files, size=500, replace=False)\n",
    "\n",
    "print('Cat datasets:', cat_train.shape, cat_val.shape, cat_test.shape)\n",
    "print('Dog datasets:', dog_train.shape, dog_val.shape, dog_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iLVQukHLp62B"
   },
   "source": [
    "## Preparing Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2HU_YFephagT"
   },
   "outputs": [],
   "source": [
    "train_dir = 'training_data'\n",
    "val_dir = 'validation_data'\n",
    "test_dir = 'test_data'\n",
    "\n",
    "train_files = np.concatenate([cat_train, dog_train])\n",
    "validate_files = np.concatenate([cat_val, dog_val])\n",
    "test_files = np.concatenate([cat_test, dog_test])\n",
    "\n",
    "os.mkdir(train_dir) if not os.path.isdir(train_dir) else None\n",
    "os.mkdir(val_dir) if not os.path.isdir(val_dir) else None\n",
    "os.mkdir(test_dir) if not os.path.isdir(test_dir) else None\n",
    "\n",
    "for fn in train_files:\n",
    "    shutil.copy(fn, train_dir)\n",
    "\n",
    "for fn in validate_files:\n",
    "    shutil.copy(fn, val_dir)\n",
    "    \n",
    "for fn in test_files:\n",
    "    shutil.copy(fn, test_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IGR17cgwhuPK"
   },
   "outputs": [],
   "source": [
    "#!ls training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "I4hFqRpEiaUA"
   },
   "outputs": [],
   "source": [
    "IMG_DIM = (150, 150)\n",
    "\n",
    "train_files = glob.glob('training_data/*')\n",
    "train_imgs = [img_to_array(load_img(img, target_size=IMG_DIM)) for img in train_files]\n",
    "train_imgs = np.array(train_imgs)\n",
    "#print(train_imgs.shape)\n",
    "train_labels = [fn.split('/')[1].split('.')[0].strip() for fn in train_files] #.split('\\\\')[1]\n",
    "print(\"Train label : \",train_labels)\n",
    "validation_files = glob.glob('validation_data/*')\n",
    "validation_imgs = [img_to_array(load_img(img, target_size=IMG_DIM)) for img in validation_files]\n",
    "validation_imgs = np.array(validation_imgs)\n",
    "validation_labels = [fn.split('/')[1].split('.')[0].strip() for fn in validation_files]\n",
    "print(\"validation label : \",validation_labels)\n",
    "\n",
    "print('Train dataset shape:', train_imgs.shape, \n",
    "      '\\tValidation dataset shape:', validation_imgs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "W0F7K_ZUjOUQ"
   },
   "outputs": [],
   "source": [
    "train_imgs_scaled = train_imgs.astype('float32')\n",
    "validation_imgs_scaled  = validation_imgs.astype('float32')\n",
    "train_imgs_scaled /= 255\n",
    "validation_imgs_scaled /= 255\n",
    "\n",
    "print(train_imgs[0].shape)\n",
    "array_to_img(train_imgs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pJWlE-wKmV-0"
   },
   "outputs": [],
   "source": [
    "batch_size = 30\n",
    "num_classes = 2\n",
    "epochs = 30\n",
    "input_shape = (150, 150, 3)\n",
    "\n",
    "# encode text category labels\n",
    "\n",
    "\n",
    "le = LabelEncoder()\n",
    "le.fit(train_labels)\n",
    "train_labels_enc = le.transform(train_labels)\n",
    "validation_labels_enc = le.transform(validation_labels)\n",
    "\n",
    "print(train_labels[1495:1505], train_labels_enc[1495:1505])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Q1jAsa36q0FQ"
   },
   "source": [
    "# Simple CNN Model from Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YkT3Gik1nL66"
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(16, kernel_size=(3, 3), activation='relu', \n",
    "                 input_shape=input_shape))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=optimizers.RMSprop(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vAIC4djTnWX8"
   },
   "outputs": [],
   "source": [
    "history = model.fit(x=train_imgs_scaled, y=train_labels_enc,\n",
    "                    validation_data=(validation_imgs_scaled, validation_labels_enc),\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pFVgn_cqo7Iy"
   },
   "outputs": [],
   "source": [
    "f, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "t = f.suptitle('Basic CNN Performance', fontsize=12)\n",
    "f.subplots_adjust(top=0.85, wspace=0.3)\n",
    "\n",
    "epoch_list = list(range(1,31))\n",
    "ax1.plot(epoch_list, history.history['acc'], label='Train Accuracy')\n",
    "ax1.plot(epoch_list, history.history['val_acc'], label='Validation Accuracy')\n",
    "ax1.set_xticks(np.arange(0, 31, 5))\n",
    "ax1.set_ylabel('Accuracy Value')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_title('Accuracy')\n",
    "l1 = ax1.legend(loc=\"best\")\n",
    "\n",
    "ax2.plot(epoch_list, history.history['loss'], label='Train Loss')\n",
    "ax2.plot(epoch_list, history.history['val_loss'], label='Validation Loss')\n",
    "ax2.set_xticks(np.arange(0, 31, 5))\n",
    "ax2.set_ylabel('Loss Value')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_title('Loss')\n",
    "l2 = ax2.legend(loc=\"best\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "F7w9EOweq_n6"
   },
   "source": [
    "## CNN Model with Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kmL3-Am1qasg"
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(16, kernel_size=(3, 3), activation='relu', \n",
    "                 input_shape=input_shape))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=optimizers.RMSprop(),\n",
    "              metrics=['accuracy'])\n",
    "              \n",
    "              \n",
    "history = model.fit(x=train_imgs_scaled, y=train_labels_enc,\n",
    "                    validation_data=(validation_imgs_scaled, validation_labels_enc),\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "16-sByveqbjw"
   },
   "outputs": [],
   "source": [
    "f, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "t = f.suptitle('Basic CNN Performance', fontsize=12)\n",
    "f.subplots_adjust(top=0.85, wspace=0.3)\n",
    "\n",
    "epoch_list = list(range(1,31))\n",
    "ax1.plot(epoch_list, history.history['acc'], label='Train Accuracy')\n",
    "ax1.plot(epoch_list, history.history['val_acc'], label='Validation Accuracy')\n",
    "ax1.set_xticks(np.arange(0, 31, 5))\n",
    "ax1.set_ylabel('Accuracy Value')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_title('Accuracy')\n",
    "l1 = ax1.legend(loc=\"best\")\n",
    "\n",
    "ax2.plot(epoch_list, history.history['loss'], label='Train Loss')\n",
    "ax2.plot(epoch_list, history.history['val_loss'], label='Validation Loss')\n",
    "ax2.set_xticks(np.arange(0, 31, 5))\n",
    "ax2.set_ylabel('Loss Value')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_title('Loss')\n",
    "l2 = ax2.legend(loc=\"best\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KNDZmKpCrsy4"
   },
   "outputs": [],
   "source": [
    "model.save(\"cats_dogs_basic_cnn.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NFOr7denrTPL"
   },
   "source": [
    "## CNN Model with Image Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Z6ds9OcertZW"
   },
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(rescale=1./255, zoom_range=0.3, rotation_range=50,\n",
    "                                   width_shift_range=0.2, height_shift_range=0.2, shear_range=0.2, \n",
    "                                   horizontal_flip=True, fill_mode='nearest')\n",
    "\n",
    "val_datagen = ImageDataGenerator(rescale=1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ESBlkT5ysToU"
   },
   "outputs": [],
   "source": [
    "img_id = 2595\n",
    "cat_generator = train_datagen.flow(train_imgs[img_id:img_id+1], train_labels[img_id:img_id+1],\n",
    "                                   batch_size=1)\n",
    "cat = [next(cat_generator) for i in range(0,5)]\n",
    "fig, ax = plt.subplots(1,5, figsize=(16, 6))\n",
    "print('Labels:', [item[1][0] for item in cat])\n",
    "l = [ax[i].imshow(cat[i][0][0]) for i in range(0,5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zuEjb4Z9847D"
   },
   "outputs": [],
   "source": [
    "img_id = 1990\n",
    "dog_generator = train_datagen.flow(train_imgs[img_id:img_id+1], train_labels[img_id:img_id+1],\n",
    "                                   batch_size=1)\n",
    "dog = [next(dog_generator) for i in range(0,5)]\n",
    "fig, ax = plt.subplots(1,5, figsize=(15, 6))\n",
    "print('Labels:', [item[1][0] for item in dog])\n",
    "l = [ax[i].imshow(dog[i][0][0]) for i in range(0,5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5JuTQ3z69RvW"
   },
   "outputs": [],
   "source": [
    "train_generator = train_datagen.flow(train_imgs, train_labels_enc, batch_size=30)\n",
    "val_generator = val_datagen.flow(validation_imgs, validation_labels_enc, batch_size=20)\n",
    "input_shape = (150, 150, 3)\n",
    "\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from keras.models import Sequential\n",
    "from keras import optimizers\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(16, kernel_size=(3, 3), activation='relu', \n",
    "                 input_shape=input_shape))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=optimizers.RMSprop(lr=1e-4),\n",
    "              metrics=['accuracy'])\n",
    "              \n",
    "history = model.fit_generator(train_generator, steps_per_epoch=100, epochs=100,\n",
    "                              validation_data=val_generator, validation_steps=50, \n",
    "                              verbose=1)              \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XUAjJGCh9wzU"
   },
   "outputs": [],
   "source": [
    "f, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "t = f.suptitle('Basic CNN Performance', fontsize=12)\n",
    "f.subplots_adjust(top=0.85, wspace=0.3)\n",
    "\n",
    "epoch_list = list(range(1,101))\n",
    "print(len(epoch_list), len(history.history['acc']))\n",
    "ax1.plot(epoch_list, history.history['acc'], label='Train Accuracy')\n",
    "ax1.plot(epoch_list, history.history['val_acc'], label='Validation Accuracy')\n",
    "ax1.set_xticks(np.arange(0, 31, 5))\n",
    "ax1.set_ylabel('Accuracy Value')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_title('Accuracy')\n",
    "l1 = ax1.legend(loc=\"best\")\n",
    "\n",
    "ax2.plot(epoch_list, history.history['loss'], label='Train Loss')\n",
    "ax2.plot(epoch_list, history.history['val_loss'], label='Validation Loss')\n",
    "ax2.set_xticks(np.arange(0, 31, 5))\n",
    "ax2.set_ylabel('Loss Value')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_title('Loss')\n",
    "l2 = ax2.legend(loc=\"best\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wdCuDznGE6El"
   },
   "outputs": [],
   "source": [
    "model.save(\"cats_dogs_cnn_img_aug.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "m7WLUOvDrdRu"
   },
   "source": [
    "## Leveraging Transfer Learning with Pre-trained CNN Models   \n",
    "\n",
    "#### Pre-trained CNN model as a Feature Extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ReEWzo5dF6R9"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "vgg = vgg16.VGG16(include_top=False, weights='imagenet', \n",
    "                                     input_shape=input_shape)\n",
    "\n",
    "output = vgg.layers[-1].output\n",
    "output = keras.layers.Flatten()(output)\n",
    "vgg_model = Model(vgg.input, output)\n",
    "\n",
    "vgg_model.trainable = False\n",
    "for layer in vgg_model.layers:\n",
    "    layer.trainable = False\n",
    "    \n",
    "import pandas as pd\n",
    "pd.set_option('max_colwidth', -1)\n",
    "layers = [(layer, layer.name, layer.trainable) for layer in vgg_model.layers]\n",
    "pd.DataFrame(layers, columns=['Layer Type', 'Layer Name', 'Layer Trainable'])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LAxA_z3wIdCJ"
   },
   "outputs": [],
   "source": [
    "bottleneck_feature_example = vgg.predict(train_imgs_scaled[0:1])\n",
    "print(bottleneck_feature_example.shape)\n",
    "plt.imshow(bottleneck_feature_example[0][:,:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "b2BXMn2rJm--"
   },
   "outputs": [],
   "source": [
    "def get_bottleneck_features(model, input_imgs):\n",
    "    features = model.predict(input_imgs, verbose=0)\n",
    "    return features\n",
    "    \n",
    "train_features_vgg = get_bottleneck_features(vgg_model, train_imgs_scaled)\n",
    "validation_features_vgg = get_bottleneck_features(vgg_model, validation_imgs_scaled)\n",
    "\n",
    "print('Train Bottleneck Features:', train_features_vgg.shape, \n",
    "      '\\tValidation Bottleneck Features:', validation_features_vgg.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SZ9aHqBbsORq"
   },
   "source": [
    "## Pre-trained CNN model as a Feature Extractor with Image Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "X596LzuDKM-V"
   },
   "outputs": [],
   "source": [
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, InputLayer\n",
    "from keras.models import Sequential\n",
    "from keras import optimizers\n",
    "\n",
    "input_shape = vgg_model.output_shape[1]\n",
    "\n",
    "model = Sequential()\n",
    "model.add(InputLayer(input_shape=(input_shape,)))\n",
    "model.add(Dense(512, activation='relu', input_dim=input_shape))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=optimizers.RMSprop(lr=1e-4),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GnKHHr0iKnov"
   },
   "outputs": [],
   "source": [
    "history = model.fit(x=train_features_vgg, y=train_labels_enc,\n",
    "                    validation_data=(validation_features_vgg, validation_labels_enc),\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JUo9v-0tK_qR"
   },
   "outputs": [],
   "source": [
    "f, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "t = f.suptitle('Basic CNN Performance', fontsize=12)\n",
    "f.subplots_adjust(top=0.85, wspace=0.3)\n",
    "\n",
    "epoch_list = list(range(1,31))\n",
    "ax1.plot(epoch_list, history.history['acc'], label='Train Accuracy')\n",
    "ax1.plot(epoch_list, history.history['val_acc'], label='Validation Accuracy')\n",
    "ax1.set_xticks(np.arange(0, 31, 5))\n",
    "ax1.set_ylabel('Accuracy Value')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_title('Accuracy')\n",
    "l1 = ax1.legend(loc=\"best\")\n",
    "\n",
    "ax2.plot(epoch_list, history.history['loss'], label='Train Loss')\n",
    "ax2.plot(epoch_list, history.history['val_loss'], label='Validation Loss')\n",
    "ax2.set_xticks(np.arange(0, 31, 5))\n",
    "ax2.set_ylabel('Loss Value')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_title('Loss')\n",
    "l2 = ax2.legend(loc=\"best\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KhdTncOULmxr"
   },
   "outputs": [],
   "source": [
    "model.save('cats_dogs_tlearn_basic_cnn.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LhKBoaoosaqj"
   },
   "source": [
    "## Pre-trained CNN model with Fine-tuning and Image Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ohHgvm8PL1qi"
   },
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(rescale=1./255, zoom_range=0.3, rotation_range=50,\n",
    "                                   width_shift_range=0.2, height_shift_range=0.2, shear_range=0.2, \n",
    "                                   horizontal_flip=True, fill_mode='nearest')\n",
    "\n",
    "val_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow(train_imgs, train_labels_enc, batch_size=30)\n",
    "val_generator = val_datagen.flow(validation_imgs, validation_labels_enc, batch_size=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QeWaUi7WMAkH"
   },
   "outputs": [],
   "source": [
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, InputLayer\n",
    "from keras.models import Sequential\n",
    "from keras import optimizers\n",
    "\n",
    "model = Sequential()\n",
    "model.add(vgg_model)\n",
    "model.add(Dense(512, activation='relu', input_dim=input_shape))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=optimizers.RMSprop(lr=2e-5),\n",
    "              metrics=['accuracy'])\n",
    "              \n",
    "history = model.fit_generator(train_generator, steps_per_epoch=100, epochs=100,\n",
    "                              validation_data=val_generator, validation_steps=50, \n",
    "                              verbose=1)              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BBaR49ktMOgM"
   },
   "outputs": [],
   "source": [
    "f, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "t = f.suptitle('Basic CNN Performance', fontsize=12)\n",
    "f.subplots_adjust(top=0.85, wspace=0.3)\n",
    "\n",
    "epoch_list = list(range(1,101))\n",
    "ax1.plot(epoch_list, history.history['acc'], label='Train Accuracy')\n",
    "ax1.plot(epoch_list, history.history['val_acc'], label='Validation Accuracy')\n",
    "ax1.set_xticks(np.arange(0, 31, 5))\n",
    "ax1.set_ylabel('Accuracy Value')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_title('Accuracy')\n",
    "l1 = ax1.legend(loc=\"best\")\n",
    "\n",
    "ax2.plot(epoch_list, history.history['loss'], label='Train Loss')\n",
    "ax2.plot(epoch_list, history.history['val_loss'], label='Validation Loss')\n",
    "ax2.set_xticks(np.arange(0, 31, 5))\n",
    "ax2.set_ylabel('Loss Value')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_title('Loss')\n",
    "l2 = ax2.legend(loc=\"best\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HySJcXFSWoUj"
   },
   "outputs": [],
   "source": [
    "model.save(\"cats_dogs_tlearn_img_aug_cnn.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LWi9YurDWz9U"
   },
   "outputs": [],
   "source": [
    "vgg_model.trainable = True\n",
    "\n",
    "set_trainable = False\n",
    "for layer in vgg_model.layers:\n",
    "    if layer.name in ['block5_conv1', 'block4_conv1']:\n",
    "        set_trainable = True\n",
    "    if set_trainable:\n",
    "        layer.trainable = True\n",
    "    else:\n",
    "        layer.trainable = False\n",
    "        \n",
    "layers = [(layer, layer.name, layer.trainable) for layer in vgg_model.layers]\n",
    "pd.DataFrame(layers, columns=['Layer Type', 'Layer Name', 'Layer Trainable'])   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eVCG2mBzW7sQ"
   },
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(rescale=1./255, zoom_range=0.3, rotation_range=50,\n",
    "                                   width_shift_range=0.2, height_shift_range=0.2, shear_range=0.2, \n",
    "                                   horizontal_flip=True, fill_mode='nearest')\n",
    "val_datagen = ImageDataGenerator(rescale=1./255)\n",
    "train_generator = train_datagen.flow(train_imgs, train_labels_enc, batch_size=30)\n",
    "val_generator = val_datagen.flow(validation_imgs, validation_labels_enc, batch_size=20)\n",
    "\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, InputLayer\n",
    "from keras.models import Sequential\n",
    "from keras import optimizers\n",
    "\n",
    "model = Sequential()\n",
    "model.add(vgg_model)\n",
    "model.add(Dense(512, activation='relu', input_dim=input_shape))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=optimizers.RMSprop(lr=1e-5),\n",
    "              metrics=['accuracy'])\n",
    "              \n",
    "history = model.fit_generator(train_generator, steps_per_epoch=100, epochs=100,\n",
    "                              validation_data=val_generator, validation_steps=50, \n",
    "                              verbose=1)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_bNtq1gJXAif"
   },
   "outputs": [],
   "source": [
    "model.save('cats_dogs_tlearn_finetune_img_aug_cnn.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "x_tFgO7ysnqa"
   },
   "source": [
    "# Evaluating our Deep Learning Models on Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_vpIeKzyjXFT"
   },
   "outputs": [],
   "source": [
    "# load dependencies\n",
    "import glob\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.preprocessing.image import load_img, img_to_array, array_to_img\n",
    "from keras.models import load_model\n",
    "#import model_evaluation_utils as meu\n",
    "%matplotlib inline\n",
    "\n",
    "# load saved models\n",
    "basic_cnn = load_model('cats_dogs_basic_cnn.h5')\n",
    "img_aug_cnn = load_model('cats_dogs_cnn_img_aug.h5')\n",
    "tl_cnn = load_model('cats_dogs_tlearn_basic_cnn.h5')\n",
    "tl_img_aug_cnn = load_model('cats_dogs_tlearn_img_aug_cnn.h5')\n",
    "tl_img_aug_finetune_cnn = load_model('cats_dogs_tlearn_finetune_img_aug_cnn.h5')\n",
    "\n",
    "# load other configurations\n",
    "IMG_DIM = (150, 150)\n",
    "input_shape = (150, 150, 3)\n",
    "num2class_label_transformer = lambda l: ['cat' if x == 0 else 'dog' for x in l]\n",
    "class2num_label_transformer = lambda l: [0 if x == 'cat' else 1 for x in l]\n",
    "\n",
    "# load VGG model for bottleneck features\n",
    "from keras.applications import vgg16\n",
    "from keras.models import Model\n",
    "import keras\n",
    "\n",
    "vgg = vgg16.VGG16(include_top=False, weights='imagenet', \n",
    "                  input_shape=input_shape)\n",
    "output = vgg.layers[-1].output\n",
    "output = keras.layers.Flatten()(output)\n",
    "vgg_model = Model(vgg.input, output)\n",
    "vgg_model.trainable = False\n",
    "\n",
    "def get_bottleneck_features(model, input_imgs):\n",
    "    features = model.predict(input_imgs, verbose=0)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "beI8fheCjbbI"
   },
   "outputs": [],
   "source": [
    "IMG_DIM = (150, 150)\n",
    "\n",
    "test_files = glob.glob('test_data/*')\n",
    "test_imgs = [img_to_array(load_img(img, target_size=IMG_DIM)) for img in test_files]\n",
    "test_imgs = np.array(test_imgs)\n",
    "test_labels = [fn.split('/')[1].split('.')[0].strip() for fn in test_files]\n",
    "\n",
    "test_imgs_scaled = test_imgs.astype('float32')\n",
    "test_imgs_scaled /= 255\n",
    "test_labels_enc = class2num_label_transformer(test_labels)\n",
    "\n",
    "print('Test dataset shape:', test_imgs.shape)\n",
    "print(test_labels[0:5], test_labels_enc[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ju8Di1YCsw__"
   },
   "source": [
    "# Model 1: Basic CNN Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KzQDbLsok-GH"
   },
   "outputs": [],
   "source": [
    "predictions = basic_cnn.predict_classes(test_imgs_scaled, verbose=0)\n",
    "predictions = num2class_label_transformer(predictions)\n",
    "display_model_performance_metrics(true_labels=test_labels, predicted_labels=predictions, \n",
    "                                      classes=list(set(test_labels)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "71wX1qiSs1-F"
   },
   "source": [
    "# Model 2: Basic CNN with Image Augmentation Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YGBdgi6GlErg"
   },
   "outputs": [],
   "source": [
    "predictions = img_aug_cnn.predict_classes(test_imgs_scaled, verbose=0)\n",
    "predictions = num2class_label_transformer(predictions)\n",
    "display_model_performance_metrics(true_labels=test_labels, predicted_labels=predictions, \n",
    "                                      classes=list(set(test_labels)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "t8DS516xs8P2"
   },
   "source": [
    "# Model 3: Transfer Learning — Pre-trained CNN as a Feature Extractor Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "luVMbl5rbNxP"
   },
   "outputs": [],
   "source": [
    "test_bottleneck_features = get_bottleneck_features(vgg_model, test_imgs_scaled)\n",
    "\n",
    "predictions = tl_cnn.predict_classes(test_bottleneck_features, verbose=0)\n",
    "predictions = num2class_label_transformer(predictions)\n",
    "display_model_performance_metrics(true_labels=test_labels, predicted_labels=predictions, \n",
    "                                      classes=list(set(test_labels)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "a9jeFpJXtDHF"
   },
   "source": [
    "# Model 4: Transfer Learning — Pre-trained CNN as a Feature Extractor with Image Augmentation Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cKlMs_9VbS96"
   },
   "outputs": [],
   "source": [
    "predictions = tl_img_aug_cnn.predict_classes(test_imgs_scaled, verbose=0)\n",
    "predictions = num2class_label_transformer(predictions)\n",
    "display_model_performance_metrics(true_labels=test_labels, predicted_labels=predictions, \n",
    "                                      classes=list(set(test_labels)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "j4HZObyftIkJ"
   },
   "source": [
    "# Model 5: Transfer Learning — Pre-trained CNN with Fine-tuning and Image Augmentation Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ae8ZXJvBbZEL"
   },
   "outputs": [],
   "source": [
    "predictions = tl_img_aug_finetune_cnn.predict_classes(test_imgs_scaled, verbose=0)\n",
    "predictions = num2class_label_transformer(predictions)\n",
    "display_model_performance_metrics(true_labels=test_labels, predicted_labels=predictions, \n",
    "                                      classes=list(set(test_labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "krC1TJBabjgg"
   },
   "outputs": [],
   "source": [
    "# worst model - basic CNN \n",
    "plot_model_roc_curve(basic_cnn, test_imgs_scaled, \n",
    "                         true_labels=test_labels_enc, \n",
    "                         class_names=[0, 1]) \n",
    "\n",
    "# best model - transfer learning with fine-tuning & image augmentation \n",
    "plot_model_roc_curve(tl_img_aug_finetune_cnn, test_imgs_scaled, \n",
    "                         true_labels=test_labels_enc, \n",
    "                         class_names=[0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Kms4uYNAbsGr"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iBQFJndWo-2u"
   },
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyMRiuFEjQdQw6+0oY9aSG6d",
   "include_colab_link": true,
   "name": "transfer_learning_dog_cat_classification_keras.ipynb",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
